{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RrV8rl9Uuub"
      },
      "outputs": [],
      "source": [
        "#This is the notebook to run the ASC pipeline proposed in the the thesis report\n",
        "#The base architecture and many functions are adapted from the following tutorials:\n",
        "#https://www.youtube.com/watch?v=c_nCjlSB1Zk\n",
        "#https://www.youtube.com/watch?v=ogEalPMUCSY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dependencies"
      ],
      "metadata": {
        "id": "O6XwDrJYsknx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install InstructorEmbedding\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence_transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install bert-score\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "!pip install torchmetrics\n",
        "!pip install accelerate\n",
        "!pip install peft"
      ],
      "metadata": {
        "id": "P5Usj25uoIbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Packages"
      ],
      "metadata": {
        "id": "XEYVs7FHMeyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "import pickle\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import bitsandbytes as bnb\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import torch\n",
        "import bert_score\n",
        "from bert_score import score\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from torchmetrics.text import TranslationEditRate\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import accelerate\n",
        "import peft\n",
        "from peft import (PeftConfig,PeftModel)\n",
        "import nltk\n",
        "from torchmetrics.text import TranslationEditRate"
      ],
      "metadata": {
        "id": "-f8BhJACtCPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load basemodel"
      ],
      "metadata": {
        "id": "zxkzr06y5ezK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#skip this if using the finetuned model\n",
        "#the llamacpp module is used to load the plain 'orca-2-7b' model because it provides efficient and fast inference\n",
        "\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path='model_path',\n",
        "    temperature=0.5,\n",
        "    max_tokens=512,\n",
        "    top_p=1,\n",
        "    n_gpu_layers=60,\n",
        "    n_batch=512,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "id": "MZ4pMzUsD-kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load finetuned model"
      ],
      "metadata": {
        "id": "wwggJ946UxkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#skip this if using the basemodel\n",
        "#the finetuned model cannot be loaded through llamacpp because it does not support the file type, the models have to be in gguf format.\n",
        "#the finetuned model could be formatted into gguf but it is a timely and intensive process and functions to load\n",
        "#the adapter model directly are provided by huggingface as done below.\n",
        "\n",
        "\n",
        "PEFT_MODEL =  '/content/drive/MyDrive/thesis data/orca 2 7b fintuned 1000 chunk' #provide path of the saved finetuned model from the finetuning notebook\n",
        "\n",
        "#create bnb config to load model in 4bit quantization to reduce computational load\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "#load base model\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path, #fetches the base model from the config file of the adapter model provided before, which is 'orca-2-7b' in this case\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "#load tokenizer for pipeline\n",
        "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#combine base orca model with the adapter layers  from fine-tuning\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
        "\n",
        "#create pipeline for direct inference usage\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=100\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "kDsRLCe4Q893"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparation of British Academic Written English Corpus (BAWE)"
      ],
      "metadata": {
        "id": "XzhKIXigsxA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#manually created folder from the \"CORPUS_ByDisc\" consisting of all xml files of the disciplines\n",
        "#Linguistics, Biological Sciences, Computer Science, Cybernetics Electronic Engineering, Engineering, Mathematics, Physics, Planning\n",
        "#because these will be the relevant disciplines for this study(to aid in the process with CSAI lectures)\n",
        "\n",
        "\n",
        "#The British Academic Written English corpus (BAWE) is used as a knowledge base for this pipeline\n",
        "#to reduce computational load the relevant files will be filtered out of it\n",
        "#the relevant files are all files from the disciplines: Linguistics, Biological Sciences, Computer Science, Cybernetics Electronic Engineering, Engineering, Mathematics, Physics, Planning\n",
        "#At first a folder called 'RELEVANT_CORPUS_XML' has to be created manually\n",
        "#this folder should contain all files from the aforementioned relevant disciplines, which can be easily constructed by using the structured folder 'CORPUS_ByDisc' after downloading the BAWE\n",
        "\n",
        "directory = \"RELEVANT_CORPUS_XML/\" #provide path to the manually created 'RELEVANT_CORPUS_XML' folder\n",
        "\n",
        "#create list with relevant file names to filter the CORPUS_TXT which is not structured by discipline\n",
        "relevantfilenames =[]\n",
        "\n",
        "#iterate through all filenames and filter out the identification code\n",
        "for filename in os.listdir(directory):\n",
        "    with open(os.path.join(directory,filename), encoding=\"utf8\") as f:\n",
        "        relevantfilenames.append(f\"{filename[-9:-4]}\") #retrieves the identification code of a file which is always at the end of the filename\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#convert data(txt files of BAWE) into documents list\n",
        "\n",
        "#define text_splitter to chunk the BAWE into bits of predefined size\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500, #set to 500 characters due to context length of 512 token\n",
        "    chunk_overlap  = 20, #some overlap to preserve more coherence\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "#provide directory to \"CORPUS_TXT\" folder of BAWE Corpus\n",
        "directory = \"CORPUS_TXT/\"\n",
        "\n",
        "docs = [] #list which will hold all chunks from the BAWE\n",
        "\n",
        "#iterate over the txt folder and use only relevant files by comparing the identification numbers\n",
        "#then split the files into chunks and append them to the docs list\n",
        "for filename in os.listdir(directory):\n",
        "    if filename[:-4] in relevantfilenames: #compare identification codes of files to only filter out relevant files\n",
        "        with open(os.path.join(directory,filename), encoding=\"utf8\") as f:\n",
        "            print(f\"{filename} works\")\n",
        "            curtext = f.read()\n",
        "            curdoc = text_splitter.create_documents([curtext])\n",
        "            for doc in curdoc:\n",
        "                docs.append(doc)\n"
      ],
      "metadata": {
        "id": "wHXjNfN3RFBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store chunked knowledge base txts from BAWE\n",
        "with open(\"WHOLE_RELEVANT_CORPUS_TXT\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(docs, fp)"
      ],
      "metadata": {
        "id": "QftvtRc-Qq2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieve Corpus txt list\n",
        "with open(\"/content/drive/MyDrive/thesis data/WHOLE_RELEVANT_CORPUS_TXT\", \"rb\") as fp:   # Unpickling\n",
        "    documents = pickle.load(fp)"
      ],
      "metadata": {
        "id": "paXx9PAnQrGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization of knowledge base\n"
      ],
      "metadata": {
        "id": "mY5AUJjsblfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to turn the chunks of BAWE into dense vector embeddings, an instructor model has to be loaded which does that\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", #this is an opensource instructor model which yields good performance\n",
        "                                                      model_kwargs={\"device\": \"cuda\"})"
      ],
      "metadata": {
        "id": "yQsu_I3Vbwjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#indicate the store path where the embeddings should be stored\n",
        "Embedding_store_path = \"/content/drive/MyDrive/thesis data/Embedding_store\""
      ],
      "metadata": {
        "id": "D85NUgtgcef6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define function which calculates embeddings and stores them\n",
        "def store_embeddings(docs, embeddings, store_name, path):\n",
        "\n",
        "    vectorStore = FAISS.from_documents(docs, embeddings) #Meta's embedding module provides this function for easy construction of embeddings\n",
        "\n",
        "    with open(f\"{path}/faiss_{store_name}.pkl\", \"wb\") as f: #embeddings are stored in a vector store to load them without having to recalculate them every time\n",
        "        pickle.dump(vectorStore, f)"
      ],
      "metadata": {
        "id": "Q9YB-xVOX2nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function loads the vector store into memory\n",
        "def load_embeddings(store_name, path):\n",
        "    with open(f\"{path}/faiss_{store_name}.pkl\", \"rb\") as f:\n",
        "        VectorStore = pickle.load(f)\n",
        "    return VectorStore"
      ],
      "metadata": {
        "id": "ZyoMbU-WbtnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#executes the creation of the embeddings\n",
        "start = time.time()\n",
        "\n",
        "store_embeddings(documents,\n",
        "                 instructor_embeddings,\n",
        "                 store_name='instructEmbeddingsRELEVANT', #provide name for embedding file\n",
        "                 path=Embedding_store_path)\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "EXejY2MrdQa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use function to actually load embeddings\n",
        "db_instructEmbedd = load_embeddings(store_name='instructEmbeddingsRELEVANT', #provide correct storing name\n",
        "                                    path=Embedding_store_path)"
      ],
      "metadata": {
        "id": "12oI4j9YelUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity search in knowledge base"
      ],
      "metadata": {
        "id": "3T1w-qgmiPC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#build a retriever on the knowledge base to retrieve similar context chunks for the prompt later\n",
        "retriever = db_instructEmbedd.as_retriever(search_kwargs={\"k\": 1}) #define the number of chunks the retriever should output, in this case it is set to 1 due to the maximum context length"
      ],
      "metadata": {
        "id": "zvVxqrFZYJf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define retrieve function to extract similar text passages from BAWE to have additional information\n",
        "def retrieve_context(query):\n",
        "  context_info = retriever.get_relevant_documents(query)\n",
        "  #extract page content of doc to delete unnecessary info from the string(source/row etc)\n",
        "  page_contents_array = [doc.page_content for doc in context_info]\n",
        "  return page_contents_array"
      ],
      "metadata": {
        "id": "ju6BrC-nvpRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up LLM to the task"
      ],
      "metadata": {
        "id": "_dHK8aASwtt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define prompt template which will be filled with the subtitle sequence and the context chunk\n",
        "template = \"\"\"\n",
        "You are tasked with correcting subtitles, which where automatically generated and\n",
        "therefore incorporate false transcriptions. Especially technical terms are often\n",
        "incorrectly transcribed. Analyse the sentences and distill the incorrect words out of the sentences and replace\n",
        "them with the correct terms. Do not make more changes.\n",
        "\n",
        "Below is the subtitle passage you should correct now:\n",
        "{subtitles}\n",
        "\n",
        "Below here is some context information to understand the context of the false transcriptions better:\n",
        "{context_info}\n",
        "\n",
        "Please output ONLY the corrected subtitle passage\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"subtitles\", \"context_info\"],\n",
        "    template=template\n",
        ")\n",
        "#chain functionality together to infer a forward pass in the llm with the appropriate prompt\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "t1gFbfoJ3wYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define function which takes in a subtitle sequence as argument\n",
        "def generate_output(subtitles):\n",
        "    context_info = retrieve_context(subtitles)\n",
        "    output = chain.run(subtitles=subtitles, context_info=context_info)\n",
        "    return output"
      ],
      "metadata": {
        "id": "8lARqa0901Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "MWKpRI1ZViCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load subtitle txts, use readlines to split into equal chunks that are equivalent in their position in the lecture\n",
        "#strip lines of newline character and drop the odd number lines because they are only timestamps and dilute the statistics because they are always the same\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_1 Panopto - DL.txt\") as f:\n",
        "  RNN_1_Panopto_NO_TS = []\n",
        "  for i, line in enumerate(f):\n",
        "    if i % 2 == 0:\n",
        "      RNN_1_Panopto_NO_TS.append(line.rstrip())\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_1 Goldstandard - DL.txt\") as f:\n",
        "  RNN_1_Goldstandard_NO_TS = []\n",
        "  for i, line in enumerate(f):\n",
        "    if i % 2 == 0:\n",
        "      RNN_1_Goldstandard_NO_TS.append(line.rstrip())\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_2 Panopto - DL.txt\") as f:\n",
        "  RNN_2_Panopto_NO_TS = []\n",
        "  for i, line in enumerate(f):\n",
        "    if i % 2 == 0:\n",
        "      RNN_2_Panopto_NO_TS.append(line.rstrip())\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_2 Goldstandard - DL.txt\") as f:\n",
        "  RNN_2_Goldstandard_NO_TS = []\n",
        "  for i, line in enumerate(f):\n",
        "    if i % 2 == 0:\n",
        "      RNN_2_Goldstandard_NO_TS.append(line.rstrip())"
      ],
      "metadata": {
        "id": "ofRzsZJO7-4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate through transcriptions and execute correction task"
      ],
      "metadata": {
        "id": "rqdMTpkzL9Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#work through data\n",
        "subtitlecorpus = RNN_1_Panopto_NO_TS #load respective corpus version WITHOUT timestamps\n",
        "RNN_1_ASC = [] #give appropriate name for output\n",
        "length = int(len(subtitlecorpus)/2) #define the length based on how many lines of the transcription should be processed at once\n",
        "\n",
        "\n",
        "for l in range(1, length + 1):\n",
        "  subtitles = ' '.join(subtitlecorpus[(l*2)-2:l*2])  # we will take 2 lines of text into one chunk due to the context length\n",
        "  ascoutput = generate_output(subtitles) # runs the correction on the provided subtitle sequence\n",
        "  RNN_1_ASC.append(ascoutput) #append LLM output to the outcome list\n",
        "  print(f\"{l}/{length} processed...\") #tracks progress\n",
        "\n"
      ],
      "metadata": {
        "id": "ucpDq3sbiB4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation metrics"
      ],
      "metadata": {
        "id": "t9WPOS0GPMCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use this to load the transcriptions created by the pipeline if they are not in memory(when revisiting the notebook for example and the calculations dont have to be made again)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_2_ASC_finetuned\", \"rb\") as fp:   #Pickling\n",
        "  RNN_2_ASC_finetuned = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_1_ASC_basemodel\", \"rb\") as fp:   #Pickling\n",
        "  RNN_1_ASC_basemodel = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_2_ASC_basemodel\", \"rb\") as fp:   #Pickling\n",
        "  RNN_2_ASC_basemodel = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_1_ASC_finetuned\", \"rb\") as fp:   #Pickling\n",
        "  RNN_1_ASC_finetuned = pickle.load(fp)"
      ],
      "metadata": {
        "id": "lWIdpkpMonq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create goldstandard 2 line version for comparison with asc outcome because the metrics compare every item from both lists and\n",
        "#since the LLM worked on 2 lines of the subtitles, the outcome has 2 lines per item\n",
        "subtitlecorpus = RNN_1_Goldstandard_NO_TS #load corpus version WITHOUT timestamps\n",
        "RNN_1_Goldstandard_2_line = [] #give appropriate name for output\n",
        "length = int(len(subtitlecorpus)/2)\n",
        "\n",
        "\n",
        "for l in range(1, length + 1): # we will take 2 lines of text into one chunk\n",
        "  subtitles = ' '.join(subtitlecorpus[(l*2)-2:l*2])\n",
        "  RNN_1_Goldstandard_2_line.append(subtitles)\n",
        "\n",
        "\n",
        "subtitlecorpus = RNN_2_Goldstandard_NO_TS #load corpus version WITHOUT timestamps\n",
        "RNN_2_Goldstandard_2_line = [] #give appropriate name for output\n",
        "length = int(len(subtitlecorpus)/2)\n",
        "\n",
        "\n",
        "for l in range(1, length + 1): # we will take 2 lines of text into one chunk\n",
        "  subtitles = ' '.join(subtitlecorpus[(l*2)-2:l*2])\n",
        "  RNN_2_Goldstandard_2_line.append(subtitles)"
      ],
      "metadata": {
        "id": "2z0iguTqHxkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BLEU\n",
        "#calculates how similar two sentences are on a word for word basis\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([RNN_1_Goldstandard_2_line], RNN_1_ASC_finetuned) #insert the corresponding data for comparison,\n",
        "#the 2 line version of the goldstandard is needed for comparison with the outputs of the pipeline because those are also incorporating 2 lines per list item\n",
        "print(BLEUscore)"
      ],
      "metadata": {
        "id": "T6yrlExwmY1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f910edfd-f886-43c5-9aa9-54b3e090193e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TER\n",
        "#calculates the edit distance between two strings\n",
        "ter = TranslationEditRate()\n",
        "ter(RNN_1_Goldstandard_2_line, RNN_1_ASC_finetuned) #insert the corresponding data for comparison"
      ],
      "metadata": {
        "id": "bScH2REv8NLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ebe2cd-9468-4f45-f875-9577992cad1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9240)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BERTSCORE\n",
        "#calculates how similar two seqeunces are on a semantic basis by making use of the embeddings created by BERT\n",
        "\n",
        "P, R, F1 = score(RNN_1_Goldstandard_2_line, RNN_1_ASC_finetuned, lang=\"en\", verbose=True)\n",
        "#score function gets provided a list of candidate and reference strings\n",
        "#and does the rest itself\n",
        "#print the averages for all scores\n",
        "print(sum(F1)/len(F1))\n",
        "print(sum(P)/len(P))\n",
        "print(sum(R)/len(R))"
      ],
      "metadata": {
        "id": "gCt5BytT8Nmr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}