{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ASC Fine-tuning Notebook"
      ],
      "metadata": {
        "id": "4dKl__PUMzlz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx2hrCfZgInc"
      },
      "outputs": [],
      "source": [
        "#This notebook is there to finetune the 'Orca-2-7b' model for the ASC pipeline\n",
        "#after this notebook is run, the created model can be used for the main ASC notebook\n",
        "\n",
        "#large chunks of the code have been adapted from this notebook:\n",
        "#https://colab.research.google.com/drive/1IqL0ay04RwNNcn5R7HzhgBqZ2lPhHloh?usp=sharing\n",
        "#which comes from this video-tutorial https://www.youtube.com/watch?v=Q9zv369Ggfk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dependencies and modules"
      ],
      "metadata": {
        "id": "yEz8Ao5IOZrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount if google drive is needed, else skip\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mngcINW4E0LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies\n",
        "\n",
        "!pip install -Uqqq pip\n",
        "!pip install -qqq bitsandbytes==0.39.0\n",
        "!pip install -qqq torch==2.1.0\n",
        "!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc\n",
        "!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f\n",
        "!pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71\n",
        "!pip install -qqq datasets==2.12.0\n",
        "!pip install -qqq loralib==0.1.1\n",
        "!pip install -qqq einops==0.6.1\n",
        "!pip install sentencepiece #if sentencepiece was not installed prior, the runtime has to be restarted"
      ],
      "metadata": {
        "id": "6mad6v5OgS_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import sentencepiece\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "gkIe9g6WgW2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create finetuning dataset"
      ],
      "metadata": {
        "id": "ZrI1qDTXMtv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load initial data\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_1 Panopto - DL.txt\") as f: #provide path to the original Panopto transription\n",
        "  RNN_1_Panopto_NO_TS = []\n",
        "  for i, line in enumerate(f):\n",
        "    if i % 2 == 0:\n",
        "      RNN_1_Panopto_NO_TS.append(line.rstrip())\n",
        "\n",
        "with open(\"/content/drive/MyDrive/thesis data/Subtitles/RNN_1 Goldstandard - DL.txt\") as f: ##provide path to the manually created goldstandard transcription\n",
        "  RNN_1_Goldstandard_NO_TS = []\n",
        "  for i, line in enumerate(f):\n",
        "    if i % 2 == 0:\n",
        "      RNN_1_Goldstandard_NO_TS.append(line.rstrip())"
      ],
      "metadata": {
        "id": "Kzp5quGYRDoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the dataset is constructed to train the model to give appropriate outputs and understand that only a single subtitle sequence is supposed to be output\n",
        "#the model is supposed to only output the provided subtitles in a corrected way\n",
        "#thats why the subtitles from the goldstandard are provided as the correct outputs, without additional text\n",
        "subtitlecorpus = RNN_1_Panopto_NO_TS #provide panopto transcript with no timestamps\n",
        "goldstandardcorpus = RNN_1_Goldstandard_NO_TS\n",
        "RNN_1_finetuning_prompts = DataFrame(columns =[\"prompt\",\"answer\"]) #set up dataframe for data\n",
        "length = len(subtitlecorpus)/3\n",
        "\n",
        "\n",
        "for l in range(1,int(len(subtitlecorpus)/3)+1): # we will take 3 lines of text into one chunk\n",
        "    subtitles = ' '.join(subtitlecorpus[(l*3)-3:l*3]) #take corresponding subtitle sequence from panopto\n",
        "    goldstandardanswer = ' '.join(RNN_1_Goldstandard_NO_TS[(l*3)-3:l*3]) #take corresponding subtitle sequence from goldstandard\n",
        "    context_info = retrieve_context(subtitles)\n",
        "    #input our information into the dataframe with prompt that will also be used in the main pipeline\n",
        "    RNN_1_finetuning_prompts.loc[l-1]= f\"\"\"\n",
        "You are tasked with correcting subtitles, which where automatically generated and\n",
        "therefore incorporate false transcriptions. Especially technical terms are often\n",
        "incorrectly transcribed. Analyse the sentences and distill the incorrect words out of the sentences and replace\n",
        "them with the correct terms. Do not make more changes.\n",
        "\n",
        "Below is the subtitle passage you should correct now:\n",
        "{subtitles}\n",
        "\n",
        "Below here is some context information to understand the context of the false transcriptions better:\n",
        "{context_info}\n",
        "\n",
        "Please output ONLY the corrected subtitle passage\n",
        "\"\"\", str(goldstandardanswer)\n",
        "    print(f\"{l}/{length} processed...\")\n",
        "\n",
        "#save the training data dataframe\n",
        "RNN_1_finetuning_prompts.to_csv('/content/drive/MyDrive/thesis data/finetuning/Finetuning_CSV.csv')"
      ],
      "metadata": {
        "id": "toNlSZkjMYW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model and prepare it for training"
      ],
      "metadata": {
        "id": "MH1D_5SpO0Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load model from huggingface\n",
        "MODEL_NAME = \"microsoft/Orca-2-7b\" #define model name\n",
        "\n",
        "\n",
        "#create bnb config to load model in 4bit quantization to reduce computational load\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "#load the model into memory\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")\n"
      ],
      "metadata": {
        "id": "Oc74Dn-CtQ3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_tokenwith t"
      ],
      "metadata": {
        "id": "4yFVxkiyKOXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable() #prevents memory issues\n",
        "model = prepare_model_for_kbit_training(model) #prepares the model for training"
      ],
      "metadata": {
        "id": "224QL-DsztWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the 'LoRA' config\n",
        "#'LoRA' is the finetuning method used and is parameter-efficient, it will reduce training time greatly\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\" # has to match model type\n",
        ")\n",
        "\n",
        "#prepare model to be trained with LoRA\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "24ZAw6KbzuLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load training dataset and prepare it for fine-tuning"
      ],
      "metadata": {
        "id": "pf4M3DquTNzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load training dataframe which was created in the beginning of this notebook\n",
        "data = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/thesis data/ASC_Finetuning_CSV.csv\")"
      ],
      "metadata": {
        "id": "OXB7DL0RzuUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shows what the data looks like before preparation\n",
        "data"
      ],
      "metadata": {
        "id": "l1DwTHXZQURu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#can be used to inspect an element of the training data\n",
        "data[\"train\"][5]"
      ],
      "metadata": {
        "id": "Hvm6fOQP366v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define functions to prepare and tokenize the data for the trainer\n",
        "\n",
        "# define function to extract information from the training data and put it into correct prompt format for the LLM\n",
        "def generate_prompt(data_point):\n",
        "  return f\"\"\"\n",
        "<human>: {data_point[\"prompt\"]}\n",
        "<assistant>: {data_point[\"answer\"]}\n",
        "\"\"\".strip()\n",
        "\n",
        "# define function to tokenize the full prompt from the preceding function\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "  full_prompt = generate_prompt(data_point)\n",
        "  tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
        "  return tokenized_full_prompt"
      ],
      "metadata": {
        "id": "MFqxWmkV38Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the initial data is being put into correct prompt format and is tokenized for training\n",
        "#it is also shuffled to break any inherent ordering\n",
        "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
      ],
      "metadata": {
        "id": "H_9o_cuF3-XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shows what the data looks like after preparation\n",
        "data"
      ],
      "metadata": {
        "id": "n1Za5cY14Bae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the fine-tuning"
      ],
      "metadata": {
        "id": "cxQuwyEbTBDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetune model\n",
        "\n",
        "#define training parameters for the trainer\n",
        "training_args = transformers.TrainingArguments(\n",
        "      per_device_train_batch_size=1,\n",
        "      gradient_accumulation_steps=4,\n",
        "      num_train_epochs=1,\n",
        "      learning_rate=2e-4,\n",
        "      fp16=True,\n",
        "      save_total_limit=3,\n",
        "      logging_steps=1,\n",
        "      output_dir=\"experiments\",\n",
        "      optim=\"paged_adamw_8bit\",\n",
        "      lr_scheduler_type=\"cosine\",\n",
        "      warmup_ratio=0.05,\n",
        ")\n",
        "#initialize trainer with predefined settings\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "#execute training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MwL_LYaC4In0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a dataname that will be used to give the final model a identifiable name\n",
        "DATA_NAME = 'ASC_Finetuning'\n",
        "#save finetuned model\n",
        "model.save_pretrained(f\"{MODEL_NAME}_finetuned_on_{DATA_NAME}\")"
      ],
      "metadata": {
        "id": "GBhQINyv4I9k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}